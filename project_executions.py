# -*- coding: utf-8 -*-
"""project_executions

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1bGyl47Oy79a_EKg5Azg__c9Lw7zuT0t3
"""

!pip install -U transformers

"""Model page: https://huggingface.co/ibm-granite/granite-3.3-2b-instruct

‚ö†Ô∏è If the generated code snippets do not work, please open an issue on either the [model repo](https://huggingface.co/ibm-granite/granite-3.3-2b-instruct)
			and/or on [huggingface.js](https://github.com/huggingface/huggingface.js/blob/main/packages/tasks/src/model-libraries-snippets.ts) üôè
"""

# Use a pipeline as a high-level helper
from transformers import pipeline

pipe = pipeline("text-generation", model="ibm-granite/granite-3.3-2b-instruct")
messages = [
    {"role": "user", "content": "Who are you?"},
]
pipe(messages)

# Load model directly
from transformers import AutoTokenizer, AutoModelForCausalLM

tokenizer = AutoTokenizer.from_pretrained("ibm-granite/granite-3.3-2b-instruct")
model = AutoModelForCausalLM.from_pretrained("ibm-granite/granite-3.3-2b-instruct")

from google.colab import files
uploaded = files.upload()

!ls -R /content

!pip install pymupdf

import fitz

!python app.py

!ls /content

